---
title: "Composite analyses v2 12-3-20"
output: html_notebook
---

#Set up
```{r warning=FALSE, message=FALSE, echo=FALSE}
# install.packages("caret", dependencies=c("Depends", "Suggests"))
#install.packages("kernlab")

library(lme4)
library(lmerTest)
library(sjPlot)
library(caret)
library(dplyr)
library(kernlab)
```
```{r}
setwd("C:\\Users\\Phuong\\OneDrive - The Ohio State University\\Libby Lab\\PCDI - Language & perspective\\COMPOSITE - Machine Learning Analyses")

ALLdf <- read.csv("Compiled data 12-2-20.csv", header=TRUE)

ALLdf$cond_p = ifelse(ALLdf$cond_p==-1, "1p", "3p")
ALLdf$cond_p = as.factor(ALLdf$cond_p)
```
###Dataset to compare algorithm predictions
```{r}
COVIDfollow_PROLIFIC <- read.csv('Data -- after Masters\\PCDI7b FILTERED 10-27-20.csv', header=TRUE)
COVIDfollow_PROLIFIC <- filter(COVIDfollow_PROLIFIC, filter_must==1)


COVIDfollow_PROLIFIC <- COVIDfollow_PROLIFIC %>% select(!c(PROLIFIC_PID:img.nost, subjDis1:IP_country, Ã¯..SubjectID, WC, presence_meaning, search_meaning)) %>%
                   mutate(Study_Code = "COVIDfollow_PROLIFIC", mem.age=NA, CDI_coded = 30 + article + prep - ppron - ipron - auxverb - conj - adverb - negate) %>%
                   rename(Text = event.description) 
COVIDfollow_PROLIFIC$SubjectID = as.numeric(COVIDfollow_PROLIFIC$SubjectID)
```



```{r}
#Streamline the dataset to only LIWC linguistic attributes
MCdf <- ALLdf %>% select(!c(SubjectID, Text, mem.age, CDI_coded, (Study_Code:allWC)))

# create a list of 80% of the rows in the original dataset we can use for training
validation_index <- createDataPartition(MCdf$cond_p, p=0.80, list=FALSE)

# select 20% of the data for validation
validation <- MCdf[-validation_index,]

# use the remaining 80% of data to training and testing the models
Trainingdf <- MCdf[validation_index,]
```

```{r}
#dimensions
dim(Trainingdf) #[1] 1154   93 #1154 cases; 93 attributes

# list types for each attribute
sapply(Trainingdf, class)

#head(Trainingdf)

levels(Trainingdf$cond_p) #"-1" "1" (1p , 3p)
```


###TRAIN ON only non-composited LIWC features.
#Take out composite scores from LIWC
```{r}
inputsATT <- Trainingdf[1:(length(Trainingdf)-1)]
outputATT <- Trainingdf[,length(Trainingdf)]
inputsATT_REDUCED <- inputsATT %>% select(!c(Analytic, Clout, Authentic, Tone, Dic))
Trainingdf_REDUCED <- Trainingdf %>% select(!c(Analytic, Clout, Authentic, Tone, Dic))

# scatterplot matrix
featurePlot(x=inputsATT_REDUCED, y=outputATT$cond_p, "ellipse")

# box and whisker plots for each attribute
featurePlot(x=inputsATT_REDUCED, y=outputATT$cond_p, plot = "box")

# density plots for each attribute by class value
scales <- list(x=list(relation="free"), y=list(relation="free"))
featurePlot(x=inputsATT_REDUCED, y=outputATT$cond_p, plot="density", scales=scales)
```

```{r}
# Run algorithms using 10-fold cross validation
control <- trainControl(method="cv", number=10)
metric <- "Accuracy"
```
```{r}
#Build test models:

# a) linear algorithms
set.seed(7)
fit.lda <- train(cond_p~., data=Trainingdf_REDUCED, method="lda", metric=metric, trControl=control)
# b) nonlinear algorithms
# CART
set.seed(7)
fit.cart <- train(cond_p~., data=Trainingdf_REDUCED, method="rpart", metric=metric, trControl=control)
# kNN
set.seed(7)
fit.knn <- train(cond_p~., data=Trainingdf_REDUCED, method="knn", metric=metric, trControl=control)
# c) advanced algorithms
# SVM
set.seed(7)
fit.svm <- train(cond_p~., data=Trainingdf_REDUCED, method="svmLinear", metric=metric, trControl=control)
# Random Forest
set.seed(7)
fit.rf <- train(cond_p~., data=Trainingdf_REDUCED, method="rf", metric=metric, trControl=control)
```

```{r}
# summarize accuracy of models
results <- resamples(list(lda=fit.lda, cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf))
summary(results)
# Call:
# summary.resamples(object = results)
# 
# Models: lda, cart, knn, svm, rf 
# Number of resamples: 10 
# 
# Accuracy 
#           Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's
# lda  0.6086957 0.6328523 0.6666792 0.6594003 0.6717016 0.7241379    0
# cart 0.5913043 0.6195652 0.6782609 0.6679385 0.7068966 0.7327586    0
# knn  0.5913043 0.6304348 0.6521739 0.6515442 0.6702586 0.7155172    0
# svm  0.5913043 0.6369565 0.6450150 0.6541529 0.6876874 0.7068966    0
# rf   0.6608696 0.6869565 0.6869565 0.6957571 0.7155172 0.7327586    0
# 
# Kappa 
#           Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's
# lda  0.2141230 0.2646880 0.3279800 0.3141267 0.3385948 0.4467958    0
# cart 0.1678214 0.2297526 0.3444243 0.3246802 0.4044903 0.4548211    0
# knn  0.1741788 0.2503110 0.2906670 0.2912472 0.3277427 0.4227986    0
# svm  0.1791951 0.2702077 0.2850878 0.3021664 0.3680449 0.4090500    0
# rf   0.3147441 0.3652738 0.3688472 0.3850553 0.4238191 0.4597356    0



# compare accuracy of models
dotplot(results)

# summarize Best Model

#Support vector machines
print(fit.svm)
# Support Vector Machines with Linear Kernel 
# 
# 1154 samples
#   87 predictor
#    2 classes: '1p', '3p' 
# 
# No pre-processing
# Resampling: Cross-Validated (10 fold) 
# Summary of sample sizes: 1038, 1038, 1038, 1039, 1038, 1039, ... 
# Resampling results:
# 
#   Accuracy   Kappa    
#   0.6541529  0.3021664
# 
# Tuning parameter 'C' was held constant at a value of 1


#Random forest
 print(fit.rf)
# Random Forest 
# 
# 1154 samples
#   87 predictor
#    2 classes: '1p', '3p' 
# 
# No pre-processing
# Resampling: Cross-Validated (10 fold) 
# Summary of sample sizes: 1038, 1038, 1038, 1039, 1038, 1039, ... 
# Resampling results across tuning parameters:
# 
#   mtry  Accuracy   Kappa    
#    2    0.6723463  0.3396307
#   44    0.6957571  0.3850553
#   87    0.6913943  0.3764051
# 
# Accuracy was used to select the optimal model using the largest value.
# The final value used for the model was mtry = 44.
```

#Quickly visualize simple models:
```{r}
plot(fit.rf)

gbmImp2 <- varImp(fit.rf, scale = FALSE)
gbmImp2

plot(gbmImp2, top = 40)
```



#Random forest with caret
```{r}
mtry <- sqrt(ncol(inputsATT_REDUCED))
#ntree: Number of trees to grow.
ntree <- 500


control <- trainControl(method='repeatedcv', 
                        number=10, 
                        repeats=3,
                        search = 'random')

#Random generate 15 mtry values with tuneLength = 15
set.seed(1)
rf_randomREDUCED <- train(cond_p~., 
                   data=Trainingdf_REDUCED,
                   method = 'rf',
                   metric = 'Accuracy',
                   tuneLength  = 15, 
                   trControl = control)

#Save model
saveRDS(rf_randomREDUCED, "./rf_randomREDUCED.rds")
# # load the model
# super_model <- readRDS("./final_model.rds")
# print(super_model)

print(rf_randomREDUCED)

# Random Forest 
# 
# 1154 samples
#   87 predictor
#    2 classes: '1p', '3p' 
# 
# No pre-processing
# Resampling: Cross-Validated (10 fold, repeated 3 times) 
# Summary of sample sizes: 1039, 1039, 1039, 1038, 1038, 1039, ... 
# Resampling results across tuning parameters:
# 
#   mtry  Accuracy   Kappa    
#    1    0.6707071  0.3337202
#    7    0.6949775  0.3846513
#   14    0.6935482  0.3806177
#   21    0.6978861  0.3897119
#   34    0.6883408  0.3704319
#   39    0.6952724  0.3842648
#   43    0.6906397  0.3746719
#   51    0.6941229  0.3815024
#   54    0.6935182  0.3805258
#   59    0.6880510  0.3694372
#   73    0.6825837  0.3577401
#   74    0.6929335  0.3789438
#   82    0.6886207  0.3702448
#   85    0.6848801  0.3623119
#   87    0.6854573  0.3637506
# 
# Accuracy was used to select the optimal model using the largest value.
# The final value used for the model was mtry = 21.

plot(rf_randomREDUCED)

gbmImp3 <- varImp(rf_randomREDUCED, scale =FALSE)
gbmImp3
#mean decrease in impurity importance of a feature is computed by measuring how effective the feature is at reducing uncertainty (classifiers) or variance (regressors) when creating decision trees within RFs

plot(gbmImp3, top = 20)
plot(gbmImp3, top = 40)

##Can also use permutation importance: Record a baseline accuracy (classifier) or R2 score (regressor) by passing a validation set or the out-of-bag (OOB) samples through the Random Forest
#Source: https://explained.ai/rf-importance/index.html
```



#Make predictions with REDUCED model
```{r}
# estimate skill of LDA on the validation dataset
validation$cond_p = as.factor(validation$cond_p)
# table(predictions)
# table(validation$cond_p)

predictions <- predict(rf_randomREDUCED, validation)
confusionMatrix(predictions, validation$cond_p)

# Confusion Matrix and Statistics
# 
#           Reference
# Prediction  1p  3p
#         1p 112  56
#         3p  38  81
#                                           
#                Accuracy : 0.6725          
#                  95% CI : (0.6149, 0.7265)
#     No Information Rate : 0.5226          
#     P-Value [Acc > NIR] : 1.867e-07       
#                                           
#                   Kappa : 0.3398          
#                                           
#  Mcnemar's Test P-Value : 0.07953         
#                                           
#             Sensitivity : 0.7467          
#             Specificity : 0.5912          
#          Pos Pred Value : 0.6667          
#          Neg Pred Value : 0.6807          
#              Prevalence : 0.5226          
#          Detection Rate : 0.3902          
#    Detection Prevalence : 0.5854          
#       Balanced Accuracy : 0.6690          
#                                           
#        'Positive' Class : 1p              
```


###Visualization function
#Source: https://shiring.github.io/machine_learning/2017/03/16/rf_plot_ggraph
```{r warning=FALSE, message=FALSE}
require(dplyr)
require(ggraph)
require(igraph)
library(randomForest)

tree_func <- function(final_model, 
                      tree_num) {
  
  # get tree by index
  tree <- randomForest::getTree(final_model, 
                                k = tree_num, 
                                labelVar = TRUE) %>%
    tibble::rownames_to_column() %>%
    # make leaf split points to NA, so the 0s won't get plotted
    mutate(`split point` = ifelse(is.na(prediction), `split point`, NA))
  
  # prepare data frame for graph
  graph_frame <- data.frame(from = rep(tree$rowname, 2),
                            to = c(tree$`left daughter`, tree$`right daughter`))
  
  # convert to graph and delete the last node that we don't want to plot
  graph <- graph_from_data_frame(graph_frame) %>%
    delete_vertices("0")
  
  # set node labels
  V(graph)$node_label <- gsub("_", " ", as.character(tree$`split var`))
  V(graph)$leaf_label <- as.character(tree$prediction)
  V(graph)$split <- as.character(round(tree$`split point`, digits = 2))
  
  # plot
  plot <- ggraph(graph, 'dendrogram') + 
    theme_bw() +
    geom_edge_link() +
    geom_node_point() +
    geom_node_text(aes(label = node_label), na.rm = TRUE, repel = TRUE) +
    geom_node_label(aes(label = split), vjust = 2.5, na.rm = TRUE, fill = "white") +
    geom_node_label(aes(label = leaf_label, fill = leaf_label), na.rm = TRUE, 
					repel = TRUE, colour = "white", fontface = "bold", show.legend = FALSE) +
    theme(panel.grid.minor = element_blank(),
          panel.grid.major = element_blank(),
          panel.background = element_blank(),
          plot.background = element_rect(fill = "white"),
          panel.border = element_blank(),
          axis.line = element_blank(),
          axis.text.x = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks = element_blank(),
          axis.title.x = element_blank(),
          axis.title.y = element_blank(),
          plot.title = element_text(size = 18))
  
  print(plot)
}
```

##Decision Tree
```{r}
decision_Tree <- tree_func(final_model = rf_randomREDUCED$finalModel, 500)
ggsave("DecisionTree.png", decision_Tree, width = 25, height = 10, units = "cm", limitsize = FALSE)
```



##Make prediction with COVID data, compare to self-report
```{r}
COVIDdf <- COVIDfollow_PROLIFIC %>% select(!c(Text:Tone, Study_Code:CDI_coded))

COVIDpredictions <- predict(rf_randomREDUCED, COVIDdf)
COVIDfollow_PROLIFIC$MC_preds <- COVIDpredictions

##Change perspective measure to binary
length(COVIDfollow_PROLIFIC$perspective) #186
table(COVIDfollow_PROLIFIC$perspective)
#  1  2  3  4  5  6 
# 64 57 24 18 19  4 

COVIDfollow_PROLIFIC <- COVIDfollow_PROLIFIC %>% mutate(perspective_binary = ifelse(perspective<4, "1p", "3p"))
COVIDfollow_PROLIFIC$perspective_binary = as.factor(COVIDfollow_PROLIFIC$perspective_binary)


confusionMatrix(COVIDpredictions, COVIDfollow_PROLIFIC$perspective_binary)
# Confusion Matrix and Statistics
# 
#           Reference
# Prediction  1p  3p
#         1p 120  26
#         3p  25  15
#                                           
#                Accuracy : 0.7258          
#                  95% CI : (0.6557, 0.7885)
#     No Information Rate : 0.7796          
#     P-Value [Acc > NIR] : 0.9658          
#                                           
#                   Kappa : 0.1951          
#                                           
#  Mcnemar's Test P-Value : 1.0000          
#                                           
#             Sensitivity : 0.8276          
#             Specificity : 0.3659          
#          Pos Pred Value : 0.8219          
#          Neg Pred Value : 0.3750          
#              Prevalence : 0.7796          
#          Detection Rate : 0.6452          
#    Detection Prevalence : 0.7849          
#       Balanced Accuracy : 0.5967          
#                                           
#        'Positive' Class : 1p            
```



##CHECK decision direction using regressions
```{r}
lmp <- function (modelobject) {
    if (class(modelobject) != "lm") stop("Not an object of class 'lm' ")
    f <- summary(modelobject)$fstatistic
    p <- pf(f[1],f[2],f[3],lower.tail=F)
    attributes(p) <- NULL
    return(p)
}
```
```{r}
features <- names(ALLdf[,which(colnames(ALLdf)=="WPS"):which(colnames(ALLdf)=="OtherP")])
Trainingdf$cond_p = ifelse(Trainingdf$cond_p=="1p", -1, 1)


cond_coefs = c()
cond_pvalue = c()
for (i in features){
  var = Trainingdf[,i]
  
  model <- lm(var ~ cond_p, data=Trainingdf)
  cond_coefs[which(features==i)]=model$coefficients[2]
  cond_pvalue[which(features==i)]=lmp(model)
}
LIWCfeatures_results <- cbind(features, cond_coefs, cond_pvalue)
LIWCfeatures_results
```




####-----------------------------###############################################
####LOOK only at imagery studies ###############################################

```{r}
ALLdf <- read.csv("Compiled data 12-2-20.csv", header=TRUE)

ALLdf$cond_p = ifelse(ALLdf$cond_p==-1, "1p", "3p")
ALLdf$cond_p = as.factor(ALLdf$cond_p)


unique(ALLdf$Study_Code)
ALLdf <- filter(ALLdf, Study_Code!="PicPrimea")
ALLdf <- filter(ALLdf, Study_Code!="PicPrimeb")
unique(ALLdf$Study_Code)
```


```{r}
#Streamline the dataset to only LIWC linguistic attributes
MCdf <- ALLdf %>% select(!c(SubjectID, Text, mem.age, CDI_coded, (Study_Code:allWC)))

# create a list of 80% of the rows in the original dataset we can use for training
validation_index <- createDataPartition(MCdf$cond_p, p=0.80, list=FALSE)

# select 20% of the data for validation
validation <- MCdf[-validation_index,]

# use the remaining 80% of data to training and testing the models
Trainingdf <- MCdf[validation_index,]
```

```{r}
#dimensions
dim(Trainingdf) #[1] 1003   93 #1154 cases; 93 attributes

# list types for each attribute
sapply(Trainingdf, class)

#head(Trainingdf)

levels(Trainingdf$cond_p) #"-1" "1" (1p , 3p)
```


###TRAIN ON only non-composited LIWC features.
#Take out composite scores from LIWC
```{r}
inputsATT <- Trainingdf[1:(length(Trainingdf)-1)]
outputATT <- Trainingdf[,length(Trainingdf)]
inputsATT_REDUCED <- inputsATT %>% select(!c(Analytic, Clout, Authentic, Tone, Dic))
Trainingdf_REDUCED <- Trainingdf %>% select(!c(Analytic, Clout, Authentic, Tone, Dic))

# scatterplot matrix
featurePlot(x=inputsATT_REDUCED, y=outputATT$cond_p, "ellipse")

# box and whisker plots for each attribute
featurePlot(x=inputsATT_REDUCED, y=outputATT$cond_p, plot = "box")

# density plots for each attribute by class value
scales <- list(x=list(relation="free"), y=list(relation="free"))
featurePlot(x=inputsATT_REDUCED, y=outputATT$cond_p, plot="density", scales=scales)
```

```{r}
# Run algorithms using 10-fold cross validation
control <- trainControl(method="cv", number=10)
metric <- "Accuracy"
```
```{r}
#Build test models:

# a) linear algorithms
set.seed(7)
fit.lda <- train(cond_p~., data=Trainingdf_REDUCED, method="lda", metric=metric, trControl=control)
# b) nonlinear algorithms
# CART
set.seed(7)
fit.cart <- train(cond_p~., data=Trainingdf_REDUCED, method="rpart", metric=metric, trControl=control)
# kNN
set.seed(7)
fit.knn <- train(cond_p~., data=Trainingdf_REDUCED, method="knn", metric=metric, trControl=control)
# c) advanced algorithms
# SVM
set.seed(7)
fit.svm <- train(cond_p~., data=Trainingdf_REDUCED, method="svmLinear", metric=metric, trControl=control)
# Random Forest
set.seed(7)
fit.rf <- train(cond_p~., data=Trainingdf_REDUCED, method="rf", metric=metric, trControl=control)
```

```{r}
# summarize accuracy of models
results <- resamples(list(lda=fit.lda, cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf))
summary(results)
# Call:
# summary.resamples(object = results)
# 
# Models: lda, cart, knn, svm, rf 
# Number of resamples: 10 
# 
# Accuracy 
#           Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's
# lda  0.6400000 0.6774505 0.6984848 0.6998851 0.7300000 0.7425743    0
# cart 0.6464646 0.6533416 0.6766337 0.6828841 0.7082426 0.7300000    0
# knn  0.6336634 0.6650248 0.6782178 0.6810435 0.6952273 0.7300000    0
# svm  0.6237624 0.6675248 0.6934343 0.6829542 0.7000000 0.7227723    0
# rf   0.6732673 0.7032178 0.7362871 0.7259455 0.7493687 0.7800000    0
# 
# Kappa 
#           Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's
# lda  0.2776886 0.3466991 0.3932073 0.3954410 0.4563005 0.4818469    0
# cart 0.2694497 0.2897248 0.3356684 0.3496431 0.4039529 0.4498778    0
# knn  0.2530482 0.3184479 0.3409509 0.3495739 0.3771259 0.4498778    0
# svm  0.2412021 0.3256939 0.3790973 0.3604391 0.3983127 0.4419890    0
# rf   0.3351287 0.3969035 0.4665432 0.4439293 0.4904653 0.5528455    0




# compare accuracy of models
dotplot(results)

# summarize Best Model

#Random forest
 print(fit.rf)
# Random Forest 
# 
# 1003 samples
#   87 predictor
#    2 classes: '1p', '3p' 
# 
# No pre-processing
# Resampling: Cross-Validated (10 fold) 
# Summary of sample sizes: 902, 902, 902, 903, 902, 903, ... 
# Resampling results across tuning parameters:
# 
#   mtry  Accuracy   Kappa    
#    2    0.7039154  0.4015949
#   44    0.7209552  0.4342670
#   87    0.7259455  0.4439293
# 
# Accuracy was used to select the optimal model using the largest value.
# The final value used for the model was mtry = 87.
```

#Quickly visualize simple models:
```{r}
plot(fit.rf)

gbmImp2 <- varImp(fit.rf, scale = FALSE)
gbmImp2

plot(gbmImp2, top = 40)
```



#Random forest with caret
```{r}
mtry <- sqrt(ncol(inputsATT_REDUCED))
#ntree: Number of trees to grow.
ntree <- 500


control <- trainControl(method='repeatedcv', 
                        number=10, 
                        repeats=3,
                        search = 'random')

#Random generate 15 mtry values with tuneLength = 15
set.seed(1)
rf_randomREDUCED <- train(cond_p~., 
                   data=Trainingdf_REDUCED,
                   method = 'rf',
                   metric = 'Accuracy',
                   tuneLength  = 15, 
                   trControl = control)

#Save model
saveRDS(rf_randomREDUCED, "./rf_imageryONLY.rds")
# # load the model
# super_model <- readRDS("./final_model.rds")
# print(super_model)

print(rf_randomREDUCED)

# Random Forest 
# 
# 1003 samples
#   87 predictor
#    2 classes: '1p', '3p' 
# 
# No pre-processing
# Resampling: Cross-Validated (10 fold, repeated 3 times) 
# Summary of sample sizes: 903, 903, 903, 902, 902, 903, ... 
# Resampling results across tuning parameters:
# 
#   mtry  Accuracy   Kappa    
#    1    0.7001653  0.3935301
#    7    0.7247863  0.4433720
#   14    0.7290900  0.4514552
#   21    0.7317368  0.4568940
#   34    0.7297467  0.4524014
#   39    0.7297469  0.4526875
#   43    0.7260733  0.4449407
#   51    0.7260603  0.4446998
#   54    0.7264069  0.4454034
#   59    0.7284137  0.4497174
#   73    0.7281067  0.4488217
#   74    0.7233936  0.4393546
#   82    0.7280834  0.4489997
#   85    0.7214333  0.4351605
#   87    0.7201232  0.4327830
# 
# Accuracy was used to select the optimal model using the largest value.
# The final value used for the model was mtry = 21.


plot(rf_randomREDUCED)

gbmImp3 <- varImp(rf_randomREDUCED, scale =FALSE)
gbmImp3
#mean decrease in impurity importance of a feature is computed by measuring how effective the feature is at reducing uncertainty (classifiers) or variance (regressors) when creating decision trees within RFs

plot(gbmImp3, top = 20)
plot(gbmImp3, top = 40)

##Can also use permutation importance: Record a baseline accuracy (classifier) or R2 score (regressor) by passing a validation set or the out-of-bag (OOB) samples through the Random Forest
#Source: https://explained.ai/rf-importance/index.html
```



#Make predictions with REDUCED model
```{r}
# estimate skill of LDA on the validation dataset
validation$cond_p = as.factor(validation$cond_p)
# table(predictions)
# table(validation$cond_p)

predictions <- predict(rf_randomREDUCED, validation)
confusionMatrix(predictions, validation$cond_p)

# Confusion Matrix and Statistics
# 
#           Reference
# Prediction  1p  3p
#         1p 110  54
#         3p  21  65
#                                           
#                Accuracy : 0.7             
#                  95% CI : (0.6391, 0.7561)
#     No Information Rate : 0.524           
#     P-Value [Acc > NIR] : 1.088e-08       
#                                           
#                   Kappa : 0.3909          
#                                           
#  Mcnemar's Test P-Value : 0.0002199       
#                                           
#             Sensitivity : 0.8397          
#             Specificity : 0.5462          
#          Pos Pred Value : 0.6707          
#          Neg Pred Value : 0.7558          
#              Prevalence : 0.5240          
#          Detection Rate : 0.4400          
#    Detection Prevalence : 0.6560          
#       Balanced Accuracy : 0.6930          
#                                           
#        'Positive' Class : 1p              
#                                                    
```


###Visualization function
#Source: https://shiring.github.io/machine_learning/2017/03/16/rf_plot_ggraph
```{r warning=FALSE, message=FALSE}
require(dplyr)
require(ggraph)
require(igraph)
library(randomForest)

tree_func <- function(final_model, 
                      tree_num) {
  
  # get tree by index
  tree <- randomForest::getTree(final_model, 
                                k = tree_num, 
                                labelVar = TRUE) %>%
    tibble::rownames_to_column() %>%
    # make leaf split points to NA, so the 0s won't get plotted
    mutate(`split point` = ifelse(is.na(prediction), `split point`, NA))
  
  # prepare data frame for graph
  graph_frame <- data.frame(from = rep(tree$rowname, 2),
                            to = c(tree$`left daughter`, tree$`right daughter`))
  
  # convert to graph and delete the last node that we don't want to plot
  graph <- graph_from_data_frame(graph_frame) %>%
    delete_vertices("0")
  
  # set node labels
  V(graph)$node_label <- gsub("_", " ", as.character(tree$`split var`))
  V(graph)$leaf_label <- as.character(tree$prediction)
  V(graph)$split <- as.character(round(tree$`split point`, digits = 2))
  
  # plot
  plot <- ggraph(graph, 'dendrogram') + 
    theme_bw() +
    geom_edge_link() +
    geom_node_point() +
    geom_node_text(aes(label = node_label), na.rm = TRUE, repel = TRUE) +
    geom_node_label(aes(label = split), vjust = 2.5, na.rm = TRUE, fill = "white") +
    geom_node_label(aes(label = leaf_label, fill = leaf_label), na.rm = TRUE, 
					repel = TRUE, colour = "white", fontface = "bold", show.legend = FALSE) +
    theme(panel.grid.minor = element_blank(),
          panel.grid.major = element_blank(),
          panel.background = element_blank(),
          plot.background = element_rect(fill = "white"),
          panel.border = element_blank(),
          axis.line = element_blank(),
          axis.text.x = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks = element_blank(),
          axis.title.x = element_blank(),
          axis.title.y = element_blank(),
          plot.title = element_text(size = 18))
  
  print(plot)
}
```

##Decision Tree
```{r}
decision_Tree <- tree_func(final_model = rf_randomREDUCED$finalModel, 500)
# ggsave("DecisionTree (imagery only).png", decision_Tree, width = 25, height = 10, units = "cm", limitsize = FALSE)
```



##Make prediction with COVID data, compare to self-report
```{r}
COVIDdf <- COVIDfollow_PROLIFIC %>% select(!c(Text:Tone, Study_Code:CDI_coded))

COVIDpredictions <- predict(rf_randomREDUCED, COVIDdf)
COVIDfollow_PROLIFIC$MC_preds <- COVIDpredictions

##Change perspective measure to binary
length(COVIDfollow_PROLIFIC$perspective) #186
table(COVIDfollow_PROLIFIC$perspective)
#  1  2  3  4  5  6 
# 64 57 24 18 19  4 

COVIDfollow_PROLIFIC <- COVIDfollow_PROLIFIC %>% mutate(perspective_binary = ifelse(perspective<4, "1p", "3p"))
COVIDfollow_PROLIFIC$perspective_binary = as.factor(COVIDfollow_PROLIFIC$perspective_binary)


confusionMatrix(COVIDpredictions, COVIDfollow_PROLIFIC$perspective_binary)
# Confusion Matrix and Statistics
# 
#           Reference
# Prediction  1p  3p
#         1p 117  27
#         3p  28  14
#                                           
#                Accuracy : 0.7043          
#                  95% CI : (0.6331, 0.7688)
#     No Information Rate : 0.7796          
#     P-Value [Acc > NIR] : 0.9936          
#                                           
#                   Kappa : 0.1471          
#                                           
#  Mcnemar's Test P-Value : 1.0000          
#                                           
#             Sensitivity : 0.8069          
#             Specificity : 0.3415          
#          Pos Pred Value : 0.8125          
#          Neg Pred Value : 0.3333          
#              Prevalence : 0.7796          
#          Detection Rate : 0.6290          
#    Detection Prevalence : 0.7742          
#       Balanced Accuracy : 0.5742          
#                                           
#        'Positive' Class : 1p              
#                                              
```



##CHECK decision direction using regressions
```{r}
lmp <- function (modelobject) {
    if (class(modelobject) != "lm") stop("Not an object of class 'lm' ")
    f <- summary(modelobject)$fstatistic
    p <- pf(f[1],f[2],f[3],lower.tail=F)
    attributes(p) <- NULL
    return(p)
}
```
```{r}
features <- names(ALLdf[,which(colnames(ALLdf)=="WPS"):which(colnames(ALLdf)=="OtherP")])
Trainingdf$cond_p = ifelse(Trainingdf$cond_p=="1p", -1, 1)


cond_coefs = c()
cond_pvalue = c()
for (i in features){
  var = Trainingdf[,i]
  
  model <- lm(var ~ cond_p, data=Trainingdf)
  cond_coefs[which(features==i)]=model$coefficients[2]
  cond_pvalue[which(features==i)]=lmp(model)
}
LIWCfeatures_results <- cbind(features, cond_coefs, cond_pvalue)
LIWCfeatures_results
```



############################LASSO REGRESSION ALGORITHM
```{r warning=FALSE, message=FALSE, echo=FALSE}
library(dplyr)
library(reshape2)
library(doc2concrete)
library(tidytext)
library(quanteda)
library(caret)
library(glmnet)
library(glmnetUtils)
library(coefplot)
```

#Rely on LIWC features
```{r}
df <- ALLdf %>% select(!c(Analytic, Clout, Authentic, Tone, Dic,mem.age, CDI_coded, Study_Code, mem_age, allWC, SubjectID, Text )) %>%
   mutate_at( vars(-cond_p), funs(c(scale(.))))  #Standardize predictor vars
  
set.seed(100) 

index = sample(1:nrow(df), 0.7*nrow(df))#70% training, 30% testing

train = df[index,] # Create the training data 
test = df[-index,] # Create the test data

dim(train) # 1008  88
dim(test) # 433 88

x_train <- data.matrix( subset(train, select=-c(cond_p)) )
y_train <- ifelse(train$cond_p == "3p", 1, 0)

x_test <- data.matrix( subset(test, select=-c(cond_p)) )
y_test <- ifelse(test$cond_p == "3p", 1, 0)
```
```{r}
# Find the best lambda using cross-validation
set.seed(123)

cv.lasso <- cv.glmnet(x_train, y_train, 
                      alpha=1, #lasso penalty
                      nfolds=10,
                      family = "binomial"
                      )

plot(cv.lasso)
plot(cv.lasso$glmnet.fit, 
     "lambda", label=FALSE)
abline(v = log(cv.lasso$lambda.min), col = "red", lty = "dashed")
abline(v = log(cv.lasso$lambda.1se), col = "red", lty = "dashed")

# identifying best lamda
min(cv.lasso$cvm)       # minimum MSE =1.230979
best_lam <- cv.lasso$lambda.min # lambda for this min MSE
best_lam #0.02273997

cv.lasso$cvm[cv.lasso$lambda == cv.lasso$lambda.1se]  # 1 st.error of min MSE =1.249569
cv.lasso$lambda.1se  # lambda for this MSE = 0.03973874
```
```{r}
# Rebuilding the model with best lamda value identified
lasso_best <- glmnet(x_train, y_train, alpha = 1, lambda = best_lam)
preds <- predict(lasso_best, s = best_lam, newx = x_test)
```
```{r}
# Make predictions on the test data
probabilities <- lasso_best %>% predict(newx = x_test)
predicted.classes <- ifelse(probabilities > 0.5, "3p", "1p")
# Model accuracy
observed.classes <- test$cond_p
mean(predicted.classes == observed.classes) #0.6859122
```
```{r}
# Inspecting beta coefficients
coef(lasso_best, s = "lambda.min") %>%
  tidy() %>%
  filter(row != "(Intercept)") %>%
  ggplot(aes(value, reorder(row, value), color = value > 0)) +
  geom_point(show.legend = FALSE) +
  ggtitle("Influential variables") +
  xlab("Coefficient") +
  ylab(NULL)
```

#n-grams tokens
```{r}
df <- ALLdf %>% select(c(cond_p, allWC, Text ))

corpus <- corpus(df$Text) #Create corpus

#Extract n-grams
tokens <- tokens(corpus, remove_punct=TRUE) %>%
    tokens_ngrams(n = 1:3) %>%
    dfm(stem = TRUE) #Stem words allowed to reduce vocab list
topfeatures(tokens, 20)
#  the  and    i    a   to   of   my   in  was   it   we   is with   on that  for   at  are   as  all 
# 6057 4601 3968 3264 3253 2080 2057 1875 1867 1318 1142 1095 1046  944  912  901  864  781  653  650 

toks_nostop <- 
    tokens(corpus, remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE) %>%
    tokens_wordstem %>%
    tokens_remove(c(stopwords("english"))) %>%
    tokens_ngrams(n = 1:3) %>%
    dfm(stem = TRUE)
topfeatures(toks_nostop, 20)
 # friend     see   peopl    walk     get    time      go     day     one    look     sit  around    veri 
 #    605     538     533     396     388     371     364     347     343     333     325     309     290 
 #   just    like    work everyon     can    play   start 
 #    279     274     248     234     228     220     219 
```

##Try with all words, including function words
```{r}
# tokensdf <- convert(tokens, to="data.frame")
# df <- merge(df, tokens)
# df <- df %>% select(-c(Text, doc_id)) %>%
#    mutate_at( vars(-cond_p), funs(c(scale(.))))  #Standardize predictor vars

tokensdf <- scale(tokens)
tokensdf = as.data.frame(as.matrix(tokensdf))
colnames(tokensdf) = make.names(colnames(tokensdf))
tokensdf$cond_p = df$cond_p
```
```{r}
#https://drsimonj.svbtle.com/ridge-regression-with-glmnet

set.seed(100) 

index = sample(1:nrow(df), 0.7*nrow(df))#70% training, 30% testing

train = tokensdf[index,] # Create the training data 
test = tokensdf[-index,] # Create the test data

dim(train) # 1008 139263
dim(test) # 433 139263

x_train <- data.matrix( subset(train, select=-c(cond_p)) )
# x_train <- as.matrix(train[,-1])
y_train <- ifelse(train$cond_p == "3p", 1, 0)

x_test <-data.matrix( subset(test, select=-c(cond_p)) )
y_test <- ifelse(test$cond_p == "3p", 1, 0)
```
```{r}
# Find the best lambda using cross-validation
set.seed(123)

cv.lasso <- cv.glmnet(x_train, y_train, 
                      alpha=1, #lasso penalty
                      nfolds=10,
                      family = "binomial"
                      )

plot(cv.lasso)
plot(cv.lasso$glmnet.fit, 
     "lambda", label=FALSE)
abline(v = log(cv.lasso$lambda.min), col = "red", lty = "dashed")
abline(v = log(cv.lasso$lambda.1se), col = "red", lty = "dashed")

# identifying best lamda
min(cv.lasso$cvm)       # minimum MSE =1.198631
best_lam <- cv.lasso$lambda.min # lambda for this min MSE
best_lam #0.02879393

cv.lasso$cvm[cv.lasso$lambda == cv.lasso$lambda.1se]  # 1 st.error of min MSE =1.219623
cv.lasso$lambda.1se  # lambda for this MSE = 0.04376422
```
```{r}
# Rebuilding the model with best lamda value identified
lasso_best <- glmnet(x_train, y_train, alpha = 1, lambda = cv.lasso$lambda.1se)
preds <- predict(lasso_best, s = cv.lasso$lambda.1se, newx = x_test)
```
```{r}
# Make predictions on the test data
probabilities <- lasso_best %>% predict(newx = x_test)
predicted.classes <- ifelse(probabilities > 0.5, "3p", "1p")
# Model accuracy
observed.classes <- test$cond_p
mean(predicted.classes == observed.classes) #LAMBDA MIN: 0.7228637, 1SE: 0.6951501
```
```{r}
# Inspecting beta coefficients
coef(lasso_best, s = "lambda.min") %>%
  tidy() %>%
  filter(row != "(Intercept)") %>%
  ggplot(aes(value, reorder(row, value), color = value > 0)) +
  geom_point(show.legend = FALSE) +
  ggtitle("Influential variables") +
  xlab("Coefficient") +
  ylab(NULL)
```


##Try with all words, including function words
```{r}
tokensdf <- scale(toks_nostop)
tokensdf = as.data.frame(as.matrix(tokensdf))
colnames(tokensdf) = make.names(colnames(tokensdf))
tokensdf$cond_p = df$cond_p
```
```{r}
#https://drsimonj.svbtle.com/ridge-regression-with-glmnet

set.seed(100) 

index = sample(1:nrow(df), 0.7*nrow(df))#70% training, 30% testing

train = tokensdf[index,] # Create the training data 
test = tokensdf[-index,] # Create the test data

dim(train) # 1008 94691
dim(test) # 433 94691

x_train <- data.matrix( subset(train, select=-c(cond_p)) )
# x_train <- as.matrix(train[,-1])
y_train <- ifelse(train$cond_p == "3p", 1, 0)

x_test <-data.matrix( subset(test, select=-c(cond_p)) )
y_test <- ifelse(test$cond_p == "3p", 1, 0)
```
```{r}
# Find the best lambda using cross-validation
set.seed(123)

cv.lasso <- cv.glmnet(x_train, y_train, 
                      alpha=1, #lasso penalty
                      nfolds=10,
                      family = "binomial"
                      )

plot(cv.lasso)
plot(cv.lasso$glmnet.fit, 
     "lambda", label=FALSE)
abline(v = log(cv.lasso$lambda.min), col = "red", lty = "dashed")
abline(v = log(cv.lasso$lambda.1se), col = "red", lty = "dashed")

# identifying best lamda
min(cv.lasso$cvm)       # minimum MSE =1.355238
best_lam <- cv.lasso$lambda.min # lambda for this min MSE
best_lam #0.02789873

cv.lasso$cvm[cv.lasso$lambda == cv.lasso$lambda.1se]  # 1 st.error of min MSE =1.361607
lambda.1se <- cv.lasso$lambda.1se  # lambda for this MSE = 0.03520421
```
```{r}
# Rebuilding the model with best lamda value identified
lasso_best <- glmnet(x_train, y_train, alpha = 1, lambda = best_lam)
preds <- predict(lasso_best, s = best_lam, newx = x_test)
```
```{r}
# Make predictions on the test data
probabilities <- lasso_best %>% predict(newx = x_test)
predicted.classes <- ifelse(probabilities > 0.5, "3p", "1p")
# Model accuracy
observed.classes <- test$cond_p
mean(predicted.classes == observed.classes) #0.5727483
```
```{r}
# Inspecting beta coefficients
coef(lasso_best, s = "best_lam") %>%
  tidy() %>%
  filter(row != "(Intercept)") %>%
  ggplot(aes(value, reorder(row, value), color = value > 0)) +
  geom_point(show.legend = FALSE) +
  ggtitle("Influential variables") +
  xlab("Coefficient") +
  ylab(NULL)
```
